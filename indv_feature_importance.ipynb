{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cujZ6PyQrR5y"
   },
   "source": [
    "# *Toxin*\n",
    "Here, we are training model on single-single features and calculating their performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing Required Libraries\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier,\n",
    "    BaggingClassifier, AdaBoostClassifier, StackingClassifier, VotingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "GradientBoosting\n",
      "LogisticRegression\n",
      "SGDClassifier\n",
      "SVC\n",
      "GaussianNB\n",
      "DecisionTreeClassifier\n",
      "MLPClassifier\n",
      "AdaBoostClassifier\n",
      "BaggingClassifier\n",
      "LinearSVC\n",
      "NuSVC\n",
      "BernoulliNB\n",
      "LinearDiscriminantAnalysis\n",
      "QuadraticDiscriminantAnalysis\n",
      "XGBClassifier\n",
      "ExtraTrees\n"
     ]
    }
   ],
   "source": [
    "# Python code to ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "df_train = pd.read_csv(\"../train_data.csv\")\n",
    "df_validation = pd.read_csv(\"../test_data.csv\")\n",
    "\n",
    "# Feature Isolation\n",
    "x_train = df_train.iloc[:, 1:-9170]\n",
    "y_train = df_train.iloc[:, -1]\n",
    "x_validation = df_validation.iloc[:, 1:-9170]\n",
    "y_validation = df_validation.iloc[:, -1]\n",
    "\n",
    "# creation performance measure function\n",
    "def perf_measure(y_actual, y_hat, thr=0.50):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    spec = 0\n",
    "    mcc = 0\n",
    "    sens = 0\n",
    "    acc = 0\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(y_hat):\n",
    "        if y_actual[i] == 1 and y_hat[i] >= thr:\n",
    "            TP += 1\n",
    "        elif y_actual[i] == 0 and y_hat[i] >= thr:\n",
    "            FP += 1\n",
    "        elif y_actual[i] == 0 and y_hat[i] < thr:\n",
    "            TN += 1\n",
    "        elif y_actual[i] == 1 and y_hat[i] < thr:\n",
    "            FN += 1\n",
    "        i += 1\n",
    "        \n",
    "    binder = TP + FN\n",
    "    nonb = TN + FP\n",
    "    total = TP + TN + FP + FN\n",
    "    \n",
    "    Pred = list(map(lambda x: 1 if x >= thr else 0, y_hat))\n",
    "    \n",
    "    if binder != 0:\n",
    "        sens = (TP / binder) * 100\n",
    "    else:\n",
    "        sens = 0\n",
    "        \n",
    "    if nonb != 0:\n",
    "        spec = TN / nonb * 100\n",
    "    else:\n",
    "        spec = 0\n",
    "        \n",
    "    acc = ((TP + TN) / total) * 100\n",
    "    f1 = 2 * TP / ((2 * TP) + FP + FN)\n",
    "    F1 = f1_score(y_actual, Pred, zero_division=0)\n",
    "    auc1 = roc_auc_score(y_actual, y_hat)\n",
    "    auprc = average_precision_score(y_actual, y_hat)\n",
    "    kappa = cohen_kappa_score(Pred, y_actual)\n",
    "    \n",
    "    if ((TP+FN)*(TP+FP)*(TN+FP)*(TN+FN)) != 0:\n",
    "        mcc = (TP * TN - FP * FN) / ((TN + FN) * (TP + FN) * (TN + FP) * (TP + FP)) ** 0.5\n",
    "    else:\n",
    "        mcc = 0\n",
    "        \n",
    "    return (TP, FP, TN, FN, sens, spec, acc, auc1, auprc, F1, kappa, mcc)\n",
    "\n",
    "\n",
    "# Classifier Initialization\n",
    "classifiers = {\n",
    "\n",
    "    \"RandomForest\": RandomForestClassifier(n_jobs=-1,  random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=42),\n",
    "    \"SGDClassifier\": SGDClassifier(loss='log', random_state=42),\n",
    "    \"SVC\": SVC(probability=True, random_state=42),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=42),\n",
    "    \"MLPClassifier\": MLPClassifier(random_state=42),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(random_state=42),\n",
    "    \"BaggingClassifier\": BaggingClassifier(random_state=42),\n",
    "    \"LinearSVC\": CalibratedClassifierCV(LinearSVC(random_state=42)), # Wrapped with CalibratedClassifierCV\n",
    "    \"NuSVC\": NuSVC(probability=True, random_state=42),\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"LinearDiscriminantAnalysis\": LinearDiscriminantAnalysis(),\n",
    "    \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "#    \"GaussianProcessClassifier\": GaussianProcessClassifier(random_state=42), #take too much time\n",
    "#    \"LabelPropagation\": LabelPropagation(),\n",
    "    \"XGBClassifier\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_jobs=-1, criterion='entropy', n_estimators=200, random_state=42),\n",
    "}\n",
    "\n",
    "# Initialize Metrics Data Dictionary\n",
    "metrics_data = {}\n",
    "\n",
    "# Running 5-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "# Looping Over Classifiers and Features\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    print(classifier_name)\n",
    "    test_model = []\n",
    "    validation_model = []    \n",
    "    for i in range(x_train.shape[1]):\n",
    "        #Extracting single feature from training data\n",
    "        X_single_feature = x_train.iloc[:, i].values.reshape(-1, 1)\n",
    "\n",
    "        test_metrics = []\n",
    "        # 5 fold-cv\n",
    "        for train_index, test_index in skf.split(X_single_feature, y_train):\n",
    "            X_fold_train, X_fold_test = X_single_feature[train_index], X_single_feature[test_index]\n",
    "            y_fold_train, y_fold_test = y_train[train_index], y_train[test_index]\n",
    "            classifier.fit(X_fold_train, y_fold_train)\n",
    "            \n",
    "            # Predict on 5 fold testing data\n",
    "            y_pred_test = classifier.predict_proba(X_fold_test)\n",
    "            metrics_test = perf_measure(y_fold_test.to_numpy(), y_pred_test[:,1])\n",
    "            test_metrics.append(metrics_test)\n",
    "        \n",
    "        # Taking mean of 5-fold testing data\n",
    "        test_df = pd.DataFrame(test_metrics).mean(axis=0)\n",
    "        test_model.append(np.array(test_df))\n",
    "\n",
    "        # Extracting single feature from validation data\n",
    "        X_single_feature_validation = x_validation.iloc[:, i].values.reshape(-1, 1)\n",
    "        y_pred_validation = classifier.predict_proba(X_single_feature_validation)\n",
    "        metrics_validation = perf_measure(y_validation.to_numpy(), y_pred_validation[:,1])                   \n",
    "        validation_model.append(metrics_validation)\n",
    "        \n",
    "    # Create DataFrames for training and test metrics\n",
    "    test_metrics_df = pd.DataFrame(np.array(test_model),\n",
    "        columns=[\"TP\", \"FP\", \"TN\", \"FN\", \"Sens\", \"Spec\", \"Accuracy\", \"AUC\", \"AUPRC\", \"F1\", \"Kappa\", \"MCC\"],\n",
    "        index=list(x_train.columns))\n",
    "\n",
    "    validation_metrics_df = pd.DataFrame(np.array(validation_model),\n",
    "        columns=[\"TP\", \"FP\", \"TN\", \"FN\", \"Sens\", \"Spec\", \"Accuracy\", \"AUC\", \"AUPRC\", \"F1\", \"Kappa\", \"MCC\"],\n",
    "        index=list(x_train.columns))\n",
    "\n",
    "    # Store metrics DataFrames in a dictionary\n",
    "    metrics_data[classifier_name] = {\n",
    "        \"Testing\": test_metrics_df,\n",
    "        \"validation\": validation_metrics_df\n",
    "    }\n",
    "\n",
    "# Now you can access the metrics for each classifier and feature using metrics_data dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory and filename where you want to save the metrics data\n",
    "metrics_data_dir = \"../metrics_data/\"\n",
    "metrics_data_filename = \"metrics_data_all_feature.csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(metrics_data_dir):\n",
    "    os.makedirs(metrics_data_dir)\n",
    "\n",
    "# Loop through the metrics_data dictionary and save each DataFrame to a CSV file\n",
    "for classifier_name, metrics in metrics_data.items():\n",
    "    for metric_type, metric_df in metrics.items():\n",
    "        # Create a directory for each classifier\n",
    "        classifier_dir = os.path.join(metrics_data_dir, classifier_name)\n",
    "        if not os.path.exists(classifier_dir):\n",
    "            os.makedirs(classifier_dir)\n",
    "        \n",
    "        # Define the full path to save the CSV file\n",
    "        csv_file_path = os.path.join(classifier_dir, f\"{metric_type}_metrics_all_feature.csv\")\n",
    "        \n",
    "        # Save the DataFrame to a CSV file\n",
    "        metric_df.to_csv(csv_file_path)\n",
    "\n",
    "print(\"Metrics data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##combined code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# creation performance measure function\n",
    "def perf_measure(y_actual, y_hat, thr=0.50):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    spec = 0\n",
    "    mcc = 0\n",
    "    sens = 0\n",
    "    acc = 0\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(y_hat):\n",
    "        if y_actual[i] == 1 and y_hat[i] >= thr:\n",
    "            TP += 1\n",
    "        elif y_actual[i] == 0 and y_hat[i] >= thr:\n",
    "            FP += 1\n",
    "        elif y_actual[i] == 0 and y_hat[i] < thr:\n",
    "            TN += 1\n",
    "        elif y_actual[i] == 1 and y_hat[i] < thr:\n",
    "            FN += 1\n",
    "        i += 1\n",
    "        \n",
    "    binder = TP + FN\n",
    "    nonb = TN + FP\n",
    "    total = TP + TN + FP + FN\n",
    "    \n",
    "    Pred = list(map(lambda x: 1 if x >= thr else 0, y_hat))\n",
    "    \n",
    "    if binder != 0:\n",
    "        sens = (TP / binder) * 100\n",
    "    else:\n",
    "        sens = 0\n",
    "        \n",
    "    if nonb != 0:\n",
    "        spec = TN / nonb * 100\n",
    "    else:\n",
    "        spec = 0\n",
    "        \n",
    "    acc = ((TP + TN) / total) * 100\n",
    "    f1 = 2 * TP / ((2 * TP) + FP + FN)\n",
    "    F1 = f1_score(y_actual, Pred, zero_division=0)\n",
    "    auc1 = roc_auc_score(y_actual, y_hat)\n",
    "    auprc = average_precision_score(y_actual, y_hat)\n",
    "    kappa = cohen_kappa_score(Pred, y_actual)\n",
    "    \n",
    "    if ((TP+FN)*(TP+FP)*(TN+FP)*(TN+FN)) != 0:\n",
    "        mcc = (TP * TN - FP * FN) / ((TN + FN) * (TP + FN) * (TN + FP) * (TP + FP)) ** 0.5\n",
    "    else:\n",
    "        mcc = 0\n",
    "        \n",
    "    return (TP, FP, TN, FN, sens, spec, acc, auc1, auprc, F1, kappa, mcc)\n",
    "\n",
    "# Specify the directory and filename where you want to save the metrics data\n",
    "metrics_data_dir = \"../metrics_data/\"\n",
    "metrics_data_filename = \"metrics_data_all_feature.csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(metrics_data_dir):\n",
    "    os.makedirs(metrics_data_dir)\n",
    "\n",
    "\n",
    "# Classifier Initialization\n",
    "classifiers = {\n",
    "    \"RandomForest\": RandomForestClassifier(criterion='entropy', max_depth=30, min_samples_split=5,\n",
    "                       n_estimators=200, n_jobs=-1, random_state=1),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_jobs=-1, criterion='entropy', n_estimators=200, random_state=42),\n",
    "    #    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "#    \"LogisticRegression\": LogisticRegression(random_state=42),\n",
    "#    \"SGDClassifier\": SGDClassifier(loss='log', random_state=42),\n",
    "#    \"SVC\": SVC(probability=True, random_state=42),\n",
    "#    \"GaussianNB\": GaussianNB(),\n",
    "#    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=42),\n",
    "#    \"MLPClassifier\": MLPClassifier(random_state=42),\n",
    "#    \"AdaBoostClassifier\": AdaBoostClassifier(random_state=42),\n",
    "#    \"BaggingClassifier\": BaggingClassifier(random_state=42),\n",
    "#    \"LinearSVC\": CalibratedClassifierCV(LinearSVC(random_state=42)), # Wrapped with CalibratedClassifierCV\n",
    "#    \"NuSVC\": NuSVC(probability=True, random_state=42),\n",
    "#    \"BernoulliNB\": BernoulliNB(),\n",
    "#    \"LinearDiscriminantAnalysis\": LinearDiscriminantAnalysis(),\n",
    "#    \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "#    \"GaussianProcessClassifier\": GaussianProcessClassifier(random_state=42), #take too much time\n",
    "#    \"LabelPropagation\": LabelPropagation(),\n",
    "#    \"XGBClassifier\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize Metrics Data Dictionary\n",
    "metrics_data = {}\n",
    "\n",
    "# Running 5-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Looping Over Classifiers and Features\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    print(classifier_name)\n",
    "    test_model = []\n",
    "    validation_model = []    \n",
    "    for i in range(x_train.shape[1]):\n",
    "        # Extracting single feature from training data\n",
    "        X_single_feature = x_train.iloc[:, i].values.reshape(-1, 1)\n",
    "\n",
    "        test_metrics = []\n",
    "        # 5 fold-cv\n",
    "        for train_index, test_index in skf.split(X_single_feature, y_train):\n",
    "            X_fold_train, X_fold_test = X_single_feature[train_index], X_single_feature[test_index]\n",
    "            y_fold_train, y_fold_test = y_train[train_index], y_train[test_index]\n",
    "            classifier.fit(X_fold_train, y_fold_train)\n",
    "            \n",
    "            # Predict on 5 fold testing data\n",
    "            y_pred_test = classifier.predict_proba(X_fold_test)\n",
    "            metrics_test = perf_measure(y_fold_test.to_numpy(), y_pred_test[:,1])\n",
    "            test_metrics.append(metrics_test)\n",
    "        \n",
    "        # Taking mean of 5-fold testing data\n",
    "        test_df = pd.DataFrame(test_metrics).mean(axis=0)\n",
    "        test_model.append(np.array(test_df))\n",
    "\n",
    "        # Extracting single feature from validation data\n",
    "        X_single_feature_validation = x_validation.iloc[:, i].values.reshape(-1, 1)\n",
    "        y_pred_validation = classifier.predict_proba(X_single_feature_validation)\n",
    "        metrics_validation = perf_measure(y_validation.to_numpy(), y_pred_validation[:,1])                   \n",
    "        validation_model.append(metrics_validation)\n",
    "        \n",
    "    # Create DataFrames for training and test metrics\n",
    "    test_metrics_df = pd.DataFrame(np.array(test_model),\n",
    "        columns=[\"TP\", \"FP\", \"TN\", \"FN\", \"Sens\", \"Spec\", \"Accuracy\", \"AUC\", \"AUPRC\", \"F1\", \"Kappa\", \"MCC\"],\n",
    "        index=list(x_train.columns))\n",
    "#return (TP, FP, TN, FN, sens, spec, acc, auc1, auprc, F1, kappa, mcc)\n",
    "    validation_metrics_df = pd.DataFrame(np.array(validation_model),\n",
    "        columns=[\"TP\", \"FP\", \"TN\", \"FN\", \"Sens\", \"Spec\", \"Accuracy\", \"AUC\", \"AUPRC\", \"F1\", \"Kappa\", \"MCC\"],\n",
    "        index=list(x_train.columns))\n",
    "\n",
    "    # Store metrics DataFrames in a dictionary\n",
    "    metrics_data[classifier_name] = {\n",
    "        \"Testing\": test_metrics_df,\n",
    "        \"Validation\": validation_metrics_df\n",
    "    }\n",
    "\n",
    "    # Save the metrics to CSV\n",
    "    classifier_dir = os.path.join(metrics_data_dir, classifier_name)\n",
    "    if not os.path.exists(classifier_dir):\n",
    "        os.makedirs(classifier_dir)\n",
    "\n",
    "    # Save training and validation metrics to CSV files\n",
    "    test_metrics_df.to_csv(os.path.join(classifier_dir, \"testing_metrics_all_feature.csv\"))\n",
    "    validation_metrics_df.to_csv(os.path.join(classifier_dir, \"validation_metrics_all_feature.csv\"))\n",
    "\n",
    "print(\"Metrics data saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
